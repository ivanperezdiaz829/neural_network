{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a21e4eb",
   "metadata": {},
   "source": [
    "Primer Trabajo FSI: Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1842",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480ef872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import kagglehub\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb060023",
   "metadata": {},
   "source": [
    "Como tenemos una carpeta con las imágenes y otra con las etiquetas (YOLO) lo primero que debemos hacer es modificar este formato para tener un fichero csv con las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4ee8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando dataset: pkdarabi/cardetection\n",
      "Dataset descargado en: C:\\Users\\Daniel\\.cache\\kagglehub\\datasets\\pkdarabi\\cardetection\\versions\\5\n",
      "\n",
      "Escaneando imágenes en: C:\\Users\\Daniel\\.cache\\kagglehub\\datasets\\pkdarabi\\cardetection\\versions\\5\\train\\images\n",
      "ERROR: No se encontró el directorio de imágenes en C:\\Users\\Daniel\\.cache\\kagglehub\\datasets\\pkdarabi\\cardetection\\versions\\5\\train\\images. Revisa la estructura del dataset de KaggleHub.\n",
      "\n",
      "--- Resumen ---\n",
      "Total de cajas delimitadoras encontradas: 0\n",
      "¡CSV creado exitosamente en: yolo_labels_dataset.csv!\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración del Dataset de KaggleHub ---\n",
    "# Reemplaza con el ID de tu dataset en KaggleHub. \n",
    "# Formato: 'owner/dataset-slug/version' o 'owner/dataset-slug'\n",
    "KAGGLE_DATASET_ID = 'pkdarabi/cardetection' \n",
    "\n",
    "# 1. Descargar el dataset usando kagglehub\n",
    "# Esto descargará y descomprimirá el dataset en una ubicación temporal/cache.\n",
    "print(f\"Descargando dataset: {KAGGLE_DATASET_ID}\")\n",
    "# 'download' devuelve la ruta local donde se guardó el dataset.\n",
    "KAGGLE_DOWNLOAD_PATH = kagglehub.dataset_download(KAGGLE_DATASET_ID)\n",
    "print(f\"Dataset descargado en: {KAGGLE_DOWNLOAD_PATH}\")\n",
    "\n",
    "# --- Configuración de Rutas (Ajustadas a la descarga) ---\n",
    "# **¡Ajuste crucial!** Reemplaza 'train/images' y 'train/labels' si tu dataset\n",
    "# tiene una estructura de subcarpeta diferente (ej: 'images', 'labels' directamente).\n",
    "# La mayoría de los datasets YOLO tienen una carpeta de 'train' o 'data'.\n",
    "\n",
    "# Definición de las rutas finales\n",
    "IMAGEN_DIR = os.path.join(KAGGLE_DOWNLOAD_PATH, 'train', 'images') \n",
    "ETIQUETAS_DIR = os.path.join(KAGGLE_DOWNLOAD_PATH, 'train', 'labels') \n",
    "CSV_SALIDA = 'yolo_labels_dataset.csv'\n",
    "EXTENSION_IMAGEN = '.jpg'\n",
    "EXTENSION_ETIQUETA = '.txt'\n",
    "\n",
    "datos = []\n",
    "\n",
    "print(f\"\\nEscaneando imágenes en: {IMAGEN_DIR}\")\n",
    "\n",
    "# 2. Iterar sobre los archivos de imagen para emparejar\n",
    "if not os.path.exists(IMAGEN_DIR):\n",
    "    print(f\"ERROR: No se encontró el directorio de imágenes en {IMAGEN_DIR}. Revisa la estructura del dataset de KaggleHub.\")\n",
    "else:\n",
    "    for archivo_imagen in os.listdir(IMAGEN_DIR):\n",
    "        if archivo_imagen.lower().endswith(EXTENSION_IMAGEN):\n",
    "            # El nombre base (sin extensión) es la clave de emparejamiento\n",
    "            nombre_base = os.path.splitext(archivo_imagen)[0]\n",
    "            \n",
    "            # 3. Construir la ruta al archivo de etiqueta\n",
    "            ruta_etiqueta = os.path.join(ETIQUETAS_DIR, nombre_base + EXTENSION_ETIQUETA)\n",
    "            \n",
    "            if os.path.exists(ruta_etiqueta):\n",
    "                \n",
    "                # 4. Leer el archivo de etiqueta\n",
    "                with open(ruta_etiqueta, 'r') as f:\n",
    "                    lineas = f.readlines()\n",
    "                \n",
    "                # 5. Procesar cada línea (cada objeto/bounding box)\n",
    "                for linea in lineas:\n",
    "                    partes = linea.strip().split()\n",
    "                    \n",
    "                    if len(partes) == 5:\n",
    "                        # El primer valor es la CLASE (entero), el resto son coordenadas (flotantes)\n",
    "                        clase_idx = int(partes[0])\n",
    "                        x_center = float(partes[1])\n",
    "                        y_center = float(partes[2])\n",
    "                        width = float(partes[3])\n",
    "                        height = float(partes[4])\n",
    "                        \n",
    "                        # 6. Almacenar los datos\n",
    "                        datos.append({\n",
    "                            'nombre_archivo': archivo_imagen,\n",
    "                            'clase_indice': clase_idx,\n",
    "                            'x_center': x_center,\n",
    "                            'y_center': y_center,\n",
    "                            'width': width,\n",
    "                            'height': height\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"¡Advertencia! Línea con formato incorrecto en {ruta_etiqueta}: {linea.strip()}\")\n",
    "            else:\n",
    "                # Nota: Es común en detección que algunas imágenes no tengan objetos (no hay archivo .txt)\n",
    "                # Si esto ocurre, la imagen no tendrá entradas en el CSV (lo cual es correcto).\n",
    "                pass\n",
    "                # print(f\"¡Advertencia! No se encontró el archivo de etiqueta para: {archivo_imagen}\")\n",
    "\n",
    "\n",
    "# 7. Crear el DataFrame y guardarlo\n",
    "df = pd.DataFrame(datos)\n",
    "df.to_csv(CSV_SALIDA, index=False)\n",
    "\n",
    "print(\"\\n--- Resumen ---\")\n",
    "print(f\"Total de cajas delimitadoras encontradas: {len(df)}\")\n",
    "if not df.empty:\n",
    "    print(\"Conteo de objetos por clase (Índice):\")\n",
    "    print(df['clase_indice'].value_counts().sort_index())\n",
    "print(f\"¡CSV creado exitosamente en: {CSV_SALIDA}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b56d9",
   "metadata": {},
   "source": [
    "Ahora ya podemos crear una instancia de la clase YOLODataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd9fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipo de objeto creado: <class '__main__.YOLODataset'>\n",
      "Número total de imágenes (longitud del dataset): 3527\n",
      "Forma del Tensor de la Imagen [0]: torch.Size([3, 416, 416])\n",
      "Forma del Tensor de Cajas [0]: torch.Size([1, 5])\n",
      "Ejemplo de Cajas (primeras 3): \n",
      "tensor([[7.0000, 0.5337, 0.3173, 0.1695, 0.3173]])\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (416, 416) \n",
    "DATA_DIR = './'\n",
    "LABELS_NAME = 'yolo_labels_dataset.csv'\n",
    "\n",
    "# 1. Definir transformaciones\n",
    "# Usamos un Compose básico para detección. \n",
    "# La normalización puede necesitar ajustarse si usas una red preentrenada.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(), # Convierte a Tensor de [0, 1]\n",
    "    # No haremos Normalización aquí, ya que a menudo es mejor no normalizar las etiquetas (las coordenadas)\n",
    "])\n",
    "\n",
    "\n",
    "# 2. Clase para Detección de Objetos\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, archivo_csv, directorio_imagenes, transform=None):\n",
    "        self.full_labels_df = pd.read_csv(archivo_csv)\n",
    "        self.directorio_imagenes = directorio_imagenes\n",
    "        self.transform = transform\n",
    "\n",
    "        # Agrupar el DataFrame por nombre de archivo. \n",
    "        # Esto permite que __len__ y __getitem__ funcionen por imagen única.\n",
    "        self.imagenes_unicas = self.full_labels_df['nombre_archivo'].unique()\n",
    "        self.labels_grouped = self.full_labels_df.groupby('nombre_archivo')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # La longitud del dataset es el número de imágenes únicas\n",
    "        return len(self.imagenes_unicas)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Obtener el nombre del archivo de la imagen para el índice actual\n",
    "        image_name = self.imagenes_unicas[idx]\n",
    "        image_path = os.path.join(self.directorio_imagenes, image_name)\n",
    "\n",
    "        # 2. Leer la imagen\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # 3. Leer las etiquetas (TODAS las cajas) asociadas a esa imagen\n",
    "        # Usamos el grupo que creamos en __init__\n",
    "        boxes_df = self.labels_grouped.get_group(image_name)\n",
    "        \n",
    "        # Seleccionamos las columnas relevantes y las convertimos a Tensor (float32)\n",
    "        # La forma del tensor de salida será: [num_boxes, 5] \n",
    "        # donde 5 es (clase_idx, x_c, y_c, w, h)\n",
    "        labels_tensor = torch.tensor(\n",
    "            boxes_df[['clase_indice', 'x_center', 'y_center', 'width', 'height']].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # 4. Transformar la imagen\n",
    "        if self.transform:\n",
    "            # NOTA: En la detección real, las coordenadas (labels_tensor) también\n",
    "            # necesitan ser reescaladas si la transformación cambia el tamaño original.\n",
    "            # Como tu transform.Resize va al mismo tamaño (416,416) y las coordenadas\n",
    "            # están normalizadas (0 a 1), esto es aceptable aquí.\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 5. Devolver la imagen y su tensor de cajas\n",
    "        # Una red de detección espera la imagen y un tensor de todas sus anotaciones.\n",
    "        return image, labels_tensor\n",
    "\n",
    "# --- Ejecución (Parte 6) ---\n",
    "\n",
    "# Crear instancia del Dataset\n",
    "ruta_csv = os.path.join(DATA_DIR, LABELS_NAME)\n",
    "ruta_imgs = os.path.join(DATA_DIR, 'images/') \n",
    "\n",
    "# Comprobación de existencia de rutas (recomendado)\n",
    "if not os.path.exists(ruta_csv) or not os.path.exists(ruta_imgs):\n",
    "    print(\"Error: Asegúrate de que el CSV y la carpeta de imágenes existen en las rutas especificadas.\")\n",
    "else:\n",
    "    dataset_yolo = YOLODataset(archivo_csv=ruta_csv, directorio_imagenes=ruta_imgs, transform=transform)\n",
    "    print(f\"\\nTipo de objeto creado: {type(dataset_yolo)}\")\n",
    "    print(f\"Número total de imágenes (longitud del dataset): {len(dataset_yolo)}\")\n",
    "\n",
    "    # Ejemplo de acceso al primer elemento:\n",
    "    # Si la imagen 0 tiene 3 objetos, image_tensor será [3, 416, 416] y labels_tensor será [3, 5]\n",
    "    if len(dataset_yolo) > 0:\n",
    "        img_tensor, boxes_tensor = dataset_yolo[0]\n",
    "        print(f\"Forma del Tensor de la Imagen [0]: {img_tensor.shape}\")\n",
    "        print(f\"Forma del Tensor de Cajas [0]: {boxes_tensor.shape}\")\n",
    "        print(f\"Ejemplo de Cajas (primeras 3): \\n{boxes_tensor[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d15e6",
   "metadata": {},
   "source": [
    "El siguiente paso será crear el DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9693eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoader creado con éxito. Número de batches: 221\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración del Loader ---\n",
    "BATCH_SIZE = 16 # Tamaño de batch comúnmente usado en detección de objetos.\n",
    "\n",
    "# 1. Definir la función de colación (collate_fn)\n",
    "def yolo_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Función de colación personalizada para detección de objetos.\n",
    "    Agrupa las imágenes en un solo tensor y las etiquetas en una lista.\n",
    "    \"\"\"\n",
    "    # Separar imágenes y etiquetas (cajas)\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch] # Cada target es un tensor [N_i, 5]\n",
    "\n",
    "    # Apilar las imágenes en un único tensor [B, C, H, W]\n",
    "    images_tensor = torch.stack(images, dim=0)\n",
    "    \n",
    "    # Devolver las etiquetas como una lista. La lista contiene los tensores de cajas\n",
    "    # de tamaño variable, uno por imagen en el batch.\n",
    "    targets_list = targets\n",
    "    \n",
    "    return images_tensor, targets_list\n",
    "\n",
    "# 2. Crear el DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset_yolo, # El dataset ya inicializado en el código anterior\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, # Barajar los datos para el entrenamiento\n",
    "    collate_fn=yolo_collate_fn, # ¡Crucial para detección de objetos!\n",
    "    num_workers=4 # Opcional: Para cargar datos más rápido\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader creado con éxito. Número de batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5279d4",
   "metadata": {},
   "source": [
    "A continuación creamos la red neuronal que entrenaremos con el dataset que hemos creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a72875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(416*416, 512) # \n",
    "        self.fc2 = nn.Linear(512, 128)    # Capa oculta con 128 neuronas\n",
    "        self.fc3 = nn.Linear(128, 15)      # Capa de salida con 10 clases (0-14)\n",
    "        self.activation = nn.Sigmoid()        # Función de activación Sigmoide\n",
    "        self.softmax = nn.Softmax(dim=1)  # Función softmax para la capa de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 416*416)            # Aplanar la imagen de 416x416 a un vector de 173056\n",
    "        #print(x.shape)                  # Mostrar la forma del tensor después de aplanarlo\n",
    "        x = self.fc1(x)                \n",
    "        x = self.activation(x)            # Función de activación Sigmoide en la capa oculta\n",
    "        #print(x.shape)                   # Mostrar la forma del tensor después de la primera capa\n",
    "        x = self.fc2(x)                  # Capa de salida\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)                   # Mostrar la forma del tensor después de la segunda capa\n",
    "        x = self.softmax(x)              # Aplicar softmax para obtener probabilidades\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15193ef",
   "metadata": {},
   "source": [
    "Definimos la función de perdida y el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93507421",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 173056]' is invalid for input of size 1568",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = SimpleNN()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Resumen del modelo\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# El modelo y las dimension de entrad de los datos\u001b[39;00m\n\u001b[32m      5\u001b[39m criterion = nn.MSELoss() \u001b[38;5;66;03m# Función de pérdida (Mean Squared Error)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchsummary\\torchsummary.py:72\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, batch_size, device)\u001b[39m\n\u001b[32m     68\u001b[39m model.apply(register_hook)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSimpleNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m416\u001b[39;49m\u001b[43m*\u001b[49m\u001b[32;43m416\u001b[39;49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# Aplanar la imagen de 416x416 a un vector de 173056\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m#print(x.shape)                  # Mostrar la forma del tensor después de aplanarlo\u001b[39;00m\n\u001b[32m     13\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc1(x)                \n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 173056]' is invalid for input of size 1568"
     ]
    }
   ],
   "source": [
    "model = SimpleNN()\n",
    "summary(model, (1, 28, 28)) # Resumen del modelo\n",
    "# El modelo y las dimension de entrad de los datos\n",
    "\n",
    "criterion = nn.MSELoss() # Función de pérdida (Mean Squared Error)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),  # Parámetros del modelo\n",
    "    lr=0.01 # ESTO ES MUY IMPORTANTE, LA TASA DE APRENDIZAJE!!!!!!!!!\n",
    ") # Optimizador (Stochastic Gradient Descent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
